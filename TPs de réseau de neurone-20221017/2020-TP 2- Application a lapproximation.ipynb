{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application à la théorie de l'approximation\n",
    "Dans cette première partie, on a une fonction $f:\\mathbb R^n \\rightarrow \\mathbb R^q$ qui n'est connue que sur un certain nombre de points $n_{data}$. L'objectif est d'approximer cette fonction $f$ en dehors des points qui sont connus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Neural as Neur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Première application \n",
    "Ici $f$ est une fonction de $[0,1]$ dans $\\mathbb{R}$ donnée par \n",
    "$x\\mapsto \\sin(6\\pi |x|^{3/2}) +x^2$. Les données d'entrées $x_i$ sont les 256 points répartis uniforméments sur $[0,1]$ et les données de sorties sont les 256 réels donnés par $y_i=f(x_i)$. créez ces données dans un tableau `x` et `y` respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(6*np.pi*np.power(x,3/2)) + np.power(x,2)\n",
    "\n",
    "x = np.linspace(0,1,256)\n",
    "y = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction d'activation que nous allons utiliser est la fonction\n",
    "$$x\\mapsto \\frac{1}{1+e^{-x}}$$\n",
    "Implémentez cette fonction d'activation ainsi que sa dérivée dans la classe `Sigmoid`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5        0.95257413 0.88079708]\n",
      "(None, array([0.        , 0.13552998, 0.20998717]))\n"
     ]
    }
   ],
   "source": [
    "import Neural_corr as Neur\n",
    "L=Neur.Sigmoid()\n",
    "x2=np.array([0,3,2])\n",
    "print(L.forward(x2))\n",
    "print(L.backward(x2))\n",
    "# Vous devez trouver\n",
    "# [0.5        0.95257413 0.88079708]\n",
    "# (None, array([0.        , 0.13552998, 0.20998717]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximation par réseau de neurones profond\n",
    "Nous allons construire 2 réseaux de neurones, le premier, noté `N` sera une couche Dense de taille $(1,12)$ suivi d'une sigmoïde et d'une couche dense $(12,1)$. Ce réseau de Neurone sera l'approximation de notre fonction $f$. Nous allons aussi créer un réseau de neurone noté `N_a` qui sera `N` suivi d'une couche de perte en norme $L^2$. Nous nous servirons de `N_a` pour l'optimisation. Créez ces deux réseaux de neurones et utilisez `N_a` pour lancer un algorithme de gradient à pas fixe avec $2000$ itérations, en faisant attention à bien régler le pas (vous verrez c'est quasiment impossible). Votre algorithme doit sortir l'évolution de la fonction objectif le long des itérations. Vous afficherez aussi le plit des vraies données et des prédictions du réseau de Neurone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (12,1) and (256,) not aligned: 1 (dim 1) != 256 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mstrdav/insa/TP_AN_OPTI/TPs de réseau de neurone-20221017/2020-TP 2- Application a lapproximation.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mstrdav/insa/TP_AN_OPTI/TPs%20de%20r%C3%A9seau%20de%20neurone-20221017/2020-TP%202-%20Application%20a%20lapproximation.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m N\u001b[39m=\u001b[39mNeur\u001b[39m.\u001b[39mNetwork([Neur\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m,\u001b[39m12\u001b[39m),Neur\u001b[39m.\u001b[39mSigmoid(),Neur\u001b[39m.\u001b[39mDense(\u001b[39m12\u001b[39m,\u001b[39m1\u001b[39m)])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mstrdav/insa/TP_AN_OPTI/TPs%20de%20r%C3%A9seau%20de%20neurone-20221017/2020-TP%202-%20Application%20a%20lapproximation.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m N_a\u001b[39m=\u001b[39mNeur\u001b[39m.\u001b[39mNetwork([N,Neur\u001b[39m.\u001b[39mLoss_L2(y)])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mstrdav/insa/TP_AN_OPTI/TPs%20de%20r%C3%A9seau%20de%20neurone-20221017/2020-TP%202-%20Application%20a%20lapproximation.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m N_a\u001b[39m.\u001b[39;49mtrain(x,y,\u001b[39m1000\u001b[39;49m,\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mstrdav/insa/TP_AN_OPTI/TPs%20de%20r%C3%A9seau%20de%20neurone-20221017/2020-TP%202-%20Application%20a%20lapproximation.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(x,y,\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m,x,N_a\u001b[39m.\u001b[39mforward(x),\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mstrdav/insa/TP_AN_OPTI/TPs%20de%20r%C3%A9seau%20de%20neurone-20221017/2020-TP%202-%20Application%20a%20lapproximation.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/insa/TP_AN_OPTI/TPs de réseau de neurone-20221017/Neural_corr.py:249\u001b[0m, in \u001b[0;36mNetwork.train\u001b[0;34m(self, X, D, nb_epochs, lr)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, X, D, nb_epochs, lr) :\n\u001b[1;32m    245\u001b[0m     \u001b[39m# X est la matrice des donnees d'entree, D est la matrice des donnees de sortie\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39m# nb_epochs est le nombre d'epochs d'apprentissage\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[39m# lr est le learning rate\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nb_epochs) :\n\u001b[0;32m--> 249\u001b[0m         Y\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(X)\n\u001b[1;32m    250\u001b[0m         loss\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msum((Y\u001b[39m-\u001b[39mD)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    251\u001b[0m         grad_local,grad_entree\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(Y\u001b[39m-\u001b[39mD)\n",
      "File \u001b[0;32m~/insa/TP_AN_OPTI/TPs de réseau de neurone-20221017/Neural_corr.py:230\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    228\u001b[0m Z\u001b[39m=\u001b[39mX\n\u001b[1;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist_layers :\n\u001b[0;32m--> 230\u001b[0m     Z\u001b[39m=\u001b[39mlayer\u001b[39m.\u001b[39;49mforward(Z)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m Z\n",
      "File \u001b[0;32m~/insa/TP_AN_OPTI/TPs de réseau de neurone-20221017/Neural_corr.py:230\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    228\u001b[0m Z\u001b[39m=\u001b[39mX\n\u001b[1;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist_layers :\n\u001b[0;32m--> 230\u001b[0m     Z\u001b[39m=\u001b[39mlayer\u001b[39m.\u001b[39;49mforward(Z)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m Z\n",
      "File \u001b[0;32m~/insa/TP_AN_OPTI/TPs de réseau de neurone-20221017/Neural_corr.py:169\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,X) :\n\u001b[1;32m    167\u001b[0m     \u001b[39m# calcul du forward, X est le vecteur des donnees d'entrees\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_X\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mcopy(X)\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mA,X) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mouter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb,np\u001b[39m.\u001b[39mones(X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (12,1) and (256,) not aligned: 1 (dim 1) != 256 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "N=Neur.Network([Neur.Dense(1,12),Neur.Sigmoid(),Neur.Dense(12,1)])\n",
    "N_a=Neur.Network([N,Neur.Loss_L2(y)])\n",
    "\n",
    "N_a.train(x,y,1000,0.1)\n",
    "plt.plot(x,y,'b',x,N_a.forward(x),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalement vous ne devez pas avoir réussi à trouver le bon pas. Nous allons donc lancer un algorithme d'optimisation de `scipy` qui s'appelle `BFGS`. Si vous créez une fonction `func(u)` qui vous calcule le coût et une fonction `nablafunc(u)` qui vous rend le gradient de la fonction `func`, alors l'algorithme de `BFGS` que nous allons utiliser se lance avec :\n",
    "`from scipy.optimize import minimize`\n",
    "`res=minimize(func, u, method='BFGS', jac=nablafunc, options={'gtol': 1e-6, 'disp': True, 'maxiter': 2000})`\n",
    "Dans le résultat `res`, il y a beaucoup d'information, mais le minimiseur est dans `res.x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-45bc04b31cc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNeur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoss_L2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "N=Neur.Network([Neur.Dense(1,12),Neur.Sigmoid(),Neur.Dense(12,1)])\n",
    "N_a=Neur.Network([N,Neur.Loss_L2(y)])\n",
    "\n",
    "\n",
    "def func(u):\n",
    "    return None\n",
    "\n",
    "def nablafunc(u):\n",
    "    return None\n",
    "\n",
    "u=N_a.get_params()\n",
    "np.random.seed(42)\n",
    "eps=1.e-4\n",
    "c=func(u)\n",
    "grad=nablafunc(u)\n",
    "for i in range(4) :\n",
    "    d=np.random.randn(u.shape[0])\n",
    "    c2=func(u+eps*d)\n",
    "    print((c2-c)/eps,np.dot(d,grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
